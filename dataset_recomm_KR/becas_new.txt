{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Recommendation Algorithm Pipeline.\n",
   
    "tags:\n",
    "- Recommendation, Becas, Metadata\n",
   
    "tldr: Recommending datasets based on metadata as of now.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "#from html.parser import HTMLParser\n",
    "#from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "import timeit\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import logging\n",
    "import pickle\n",
    "from scipy import sparse\n",
    "import json\n",
    "from sys import getsizeof\n",
    "#from itertools import chain\n",
    "#GEO_URL = \"http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=\"\n",
    "#import becas\n",
    "#import pickle\n",
    "#import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myclient = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# # Creating the database\n",
    "# gsedb = myclient[\"Metadata_becas_gse\"]\n",
    "\n",
    "# # Creating the collection\n",
    "# #gsecol = gsedb['gse_ids']\n",
    "# gsecol_new = gsedb['gse_ids_new']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the exported mongodb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('metadata_becas_new_final.json') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get the metadata from mongodb or json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gse id and identifiers\n",
    "document_id_and_identifier = {}\n",
    "\n",
    "# Gse id name of the identifier and the identifier\n",
    "document_id_name_and_identifier = {}\n",
    "\n",
    "# list of entitiy identifiers\n",
    "flat_list_identifier_ent = []\n",
    "\n",
    "# list of ids identifier\n",
    "flat_list_identifier_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(gse_id, flat_list_identifier_ent, flat_list_identifier_ids, document_id_and_identifier):\n",
    "    \n",
    "    identifier_ent = []\n",
    "    identifier_ids = []\n",
    "    \n",
    "    name_and_identifier = {}\n",
    "    \n",
    "    # for reading directly from database\n",
    "    #doc = gsecol_new.find_one({'gse_id': gse_id})\n",
    "    \n",
    "    \n",
    "    # for reading from json file of the database\n",
    "    for document in data:\n",
    "        if document['gse_id'] == gse_id:\n",
    "            doc = document\n",
    "#             else: \n",
    "#                 print('no record for {}'.format(gse_id))\n",
    "            #print(doc)\n",
    "\n",
    "    for ent_no in range(len(doc['entities'])):\n",
    "\n",
    "        name = doc['entities'][ent_no]['name']\n",
    "        identifier = doc['entities'][ent_no]['identifier']\n",
    "\n",
    "        name_and_identifier[name] = identifier\n",
    "\n",
    "        identifier_ent.append(identifier)\n",
    "\n",
    "    for id_no in range(len(doc['ids'])):\n",
    "\n",
    "        tag_name = doc['ids'][id_no]['tag_name']\n",
    "        identity = doc['ids'][id_no]['identity']\n",
    "\n",
    "        name_and_identifier[tag_name] = identity\n",
    "\n",
    "        identifier_ids.append(identity)\n",
    "\n",
    "    flat_identifier_ent = [item for sublist in identifier_ent for item in sublist]\n",
    "#     print(flat_identifier_ent)\n",
    "#     print('-----')\n",
    "#     print(identifier_ids)\n",
    "\n",
    "\n",
    "    document_id_name_and_identifier[gse_id] = name_and_identifier\n",
    "\n",
    "    document_id_and_identifier[gse_id] = []\n",
    "    document_id_and_identifier[gse_id].extend(flat_identifier_ent)\n",
    "    document_id_and_identifier[gse_id].extend(identifier_ids)\n",
    "\n",
    "    flat_list_identifier_ent.extend(flat_identifier_ent)\n",
    "    flat_list_identifier_ids.extend(identifier_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_metadata(gse_id, flat_list_identifier_ent, flat_list_identifier_ids, document_id_and_identifier):\n",
    "    \n",
    "#     doc = gsecol.find_one({'gse_id': gse_id})\n",
    "    \n",
    "#     name_ent, identifier_ent = zip(*doc['entities'].items())\n",
    "#     name_ids, identifier_ids = zip(*doc['ids'].items())\n",
    "    \n",
    "#     print(name_ent)\n",
    "#     print(identifier_ent)\n",
    "#     print(name_ids)\n",
    "#     print(identifier_ids['identity'])\n",
    "    \n",
    "#     #name_and_identifier = {}\n",
    "# #     \n",
    "# #     for i in range(len(name_ent)):\n",
    "# #         name_and_identifier[name_ent[i]] = identifier_ent[i]\n",
    "#     name_and_identifier = dict(zip(name_ent, identifier_ent))\n",
    "    \n",
    "# #     for i in range(len(name_ids)):\n",
    "# #         #flat_identifier_ids = [sublist['identity'] for sublist in identifier_ids]\n",
    "# #         name_and_identifier[name_ids[i]] = identifier_ids[i]['identity']\n",
    "    \n",
    "#     name_and_identifier = zip(name_ids, identifier_ids)\n",
    "    \n",
    "#     flat_identifier_ent = [item for sublist in identifier_ent for item in sublist]\n",
    "#     flat_identifier_ids = [sublist['identity'] for sublist in identifier_ids]\n",
    "    \n",
    "# #     print(flat_identifier_ent)\n",
    "# #     print(flat_identifier_ids)\n",
    "    \n",
    "# #     flat_list_identifiers_ent_ids = flat_identifier_ent + flat_identifier_ids\n",
    "# #     print(flat_list_identifiers_ent_ids)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #name_and_identifier = dict(zip(ent_ids, flat_list_identifiers_ent_ids))\n",
    "#     #print(len(ent_ids))\n",
    "#     #print(len(flat_list_identifiers_ent_ids))\n",
    "    \n",
    "#     document_id_and_identifier[gse_id] = []\n",
    "#     document_id_and_identifier[gse_id].extend(flat_identifier_ent)\n",
    "#     document_id_and_identifier[gse_id].extend(flat_identifier_ids)\n",
    "    \n",
    "    \n",
    "#     #print(name_and_identifier)\n",
    "    \n",
    "#     #document_id_name_and_identifier[gse_id] = name_and_identifier\n",
    "#     #print(document_id_name_and_identifier[gse_id])\n",
    "    \n",
    "#     flat_list_identifier_ent.extend(flat_identifier_ent)\n",
    "#     flat_list_identifier_ids.extend(flat_identifier_ids)\n",
    "    \n",
    "    \n",
    "    \n",
    "# #     print(name_and_identifier)\n",
    "#     document_id_name_and_identifier[gse_id] = name_and_identifier\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_metadata(gse_id, flat_list_identifier_ent, flat_list_identifier_ids, document_id_and_identifier):\n",
    "    \n",
    "#     # find the document of corresponding Gse id\n",
    "#     doc = gsecol.find_one({'gse_id': gse_id})\n",
    "    \n",
    "    \n",
    "#     # name of the entity and identifier of the entity\n",
    "#     name_ent, identifier_ent = zip(*doc['entities'].items())\n",
    "#     # name of the ids and identifier of the ids\n",
    "#     name_ids, identifier_ids = zip(*doc['ids'].items())\n",
    "    \n",
    "#     # name and their identifier\n",
    "#     name_and_identifier = {}\n",
    "    \n",
    "#     # filling the name and identifier dict with keys as name entity and values as idenifier entity\n",
    "#     for i in range(len(name_ent)):\n",
    "        \n",
    "#         name_and_identifier[name_ent[i]] = identifier_ent[i]\n",
    "    \n",
    "#     # filling the name and identifier dict with keys as name ids and values as idenifier ids\n",
    "#     for i in range(len(name_ids)):\n",
    "        \n",
    "#         name_and_identifier[name_ids[i]] = identifier_ids[i]['identity']\n",
    "    \n",
    "#     # filling the document id name and identifier dict with keys as gse id and values as name and identifier from above\n",
    "#     document_id_name_and_identifier[gse_id] = name_and_identifier\n",
    "    \n",
    "#     # list of identifier entities\n",
    "#     flat_identifier_ent = [item for sublist in identifier_ent for item in sublist]\n",
    "#     # list of identifier ids\n",
    "#     flat_identifier_ids = [sublist['identity'] for sublist in identifier_ids]\n",
    "    \n",
    "#     # filling the document id and identifier with keys as gse id and values as identifier entities and identifier ids\n",
    "#     document_id_and_identifier[gse_id] = []\n",
    "#     document_id_and_identifier[gse_id].extend(flat_identifier_ent)\n",
    "#     document_id_and_identifier[gse_id].extend(flat_identifier_ids)\n",
    "    \n",
    "#     # list of identifier entities and identifier ids\n",
    "#     flat_list_identifier_ent.extend(flat_identifier_ent)\n",
    "#     flat_list_identifier_ids.extend(flat_identifier_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the metadata from the documents for creating the columns of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "# reading the file having gse ids \n",
    "gse_list = pd.read_csv('gse_final_list.txt', header = None)\n",
    "#gse_list = ['GSE13690', 'GSE29799', 'GSE45430', 'GSE28823', 'GSE24797', 'GSE40939', 'GSE27513', 'GSE47813']\n",
    "\n",
    "# no. of documents with nothing in it\n",
    "no_documents_count = 0\n",
    "# no. of valid documents with information\n",
    "documents_count = 0\n",
    "\n",
    "# gse ids for which we got the metadata \n",
    "valid_geo_ids = []\n",
    "# gse ids for which we didn't got the metadata\n",
    "invalid_geo_ids = []\n",
    "\n",
    "# iterating over gse ids\n",
    "for gse_id in gse_list[0]:#[0]\n",
    "    #print(count)\n",
    "#     if count >= 2:\n",
    "#         break\n",
    "    try:\n",
    "        # getting the metadata\n",
    "        get_metadata(gse_id, flat_list_identifier_ent, flat_list_identifier_ids, document_id_and_identifier)\n",
    "        # increasing the count of valid documents \n",
    "        documents_count+=1\n",
    "        # keeping track of valid gse ids \n",
    "        valid_geo_ids.append(gse_id)\n",
    "        #print('getting metadata from {}'.format(gse_id))\n",
    "        #count += 1\n",
    "        \n",
    "    except TypeError:\n",
    "        # increasing the invalid documents\n",
    "        no_documents_count+=1\n",
    "        # keeping track of invalid gse ids\n",
    "        invalid_geo_ids.append(gse_id)\n",
    "        print('no metadata from {}'.format(gse_id))\n",
    "        \n",
    "        continue\n",
    "        \n",
    "    except ValueError:\n",
    "        # increasing the invalid documents\n",
    "        no_documents_count+=1\n",
    "        # keeping track of invalid gse ids\n",
    "        invalid_geo_ids.append(gse_id)\n",
    "        print('no metadata from {}'.format(gse_id))\n",
    "        \n",
    "        continue\n",
    "    \n",
    "    except :\n",
    "        # increasing the invalid documents\n",
    "        no_documents_count+=1\n",
    "        # keeping track of invalid gse ids\n",
    "        invalid_geo_ids.append(gse_id)\n",
    "        print('no metadata from {}'.format(gse_id))\n",
    "        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gse id metadata name and corresponding identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_id_name_and_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_id_and_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_list_identifier_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_list_identifier_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing the doc name identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"doc_id_name_identifier_dict_new_final\"\n",
    "# fileObject = open(file_Name,'wb') \n",
    "\n",
    "# pickle.dump(document_id_name_and_identifier, fileObject)  \n",
    "# fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open(file_Name,'rb')\n",
    "\n",
    "document_id_name_and_identifier = pickle.load(fileObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metadata name and their corresponding identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name and identifier dict\n",
    "merged_name_id_dict = {}\n",
    "\n",
    "# keeping the name as keys and their corresponding identifiers as values\n",
    "for gse in document_id_name_and_identifier.keys():\n",
    "    \n",
    "    if len(merged_name_id_dict.keys()) == 0:\n",
    "        # if key is new \n",
    "        merged_name_id_dict = document_id_name_and_identifier[gse]\n",
    "    else:\n",
    "        # if key already exists\n",
    "        merged_name_id_dict.update(document_id_name_and_identifier[gse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_name_id_dict['hematopoietic stem cells']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing the merged name id dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"name_identifier_dict_new_final\"\n",
    "# fileObject = open(file_Name,'wb') \n",
    "\n",
    "# pickle.dump(merged_name_id_dict, fileObject)  \n",
    "# fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open(file_Name,'rb')\n",
    "\n",
    "merged_name_id_dict = pickle.load(fileObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idenifiers and their common metadata names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifiers and the common names which belongs to their identifiers\n",
    "common_identifier_dict = {}\n",
    "\n",
    "count = 0\n",
    "\n",
    "# iterating over keys of document id name and identifier\n",
    "for gse in document_id_name_and_identifier.keys():\n",
    "    \n",
    "    # iterating over key and values of document id name and identifier\n",
    "    for key, value in document_id_name_and_identifier[gse].items():\n",
    "        \n",
    "        # if it's a string, not a list keep it in a list\n",
    "        if isinstance(value, str):\n",
    "            value_list = []\n",
    "            value_list.append(value)\n",
    "            \n",
    "            # iterating over elements of the above list\n",
    "            for val in value_list:\n",
    "                \n",
    "                # if element is new, then create\n",
    "                if val not in common_identifier_dict.keys():\n",
    "                    common_identifier_dict[val] = []\n",
    "                    common_identifier_dict[val].append(key)\n",
    "                # if element exists then append\n",
    "                else:\n",
    "                    common_identifier_dict[val].append(key)\n",
    "        \n",
    "        # if it's a list\n",
    "        else:\n",
    "            \n",
    "            # iterating over elements of the above list\n",
    "            for val in value:\n",
    "                \n",
    "                # if element is new, then create\n",
    "                if val not in common_identifier_dict.keys():\n",
    "                    common_identifier_dict[val] = []\n",
    "                 \n",
    "                    common_identifier_dict[val].append(key)\n",
    "                # if element exists then append\n",
    "                else:\n",
    "                    common_identifier_dict[val].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common_identifier_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing the common identifier dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"common_identifier_dict_new_final\"\n",
    "# fileObject = open(file_Name,'wb') \n",
    "\n",
    "# pickle.dump(common_identifier_dict, fileObject)  \n",
    "# fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open(file_Name,'rb')\n",
    "\n",
    "common_identifier_dict = pickle.load(fileObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the KAT2A and GCN5 common identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_identifier_dict['UNIPROT:Q92830:T116:PRGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_identifier_dict['UNIPROT:Q92831:T116:PRGE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gse ids and their corresponding identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_id_and_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_list_identifier_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_list_identifier_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('no. of documents with metadata: {}'.format(documents_count))\n",
    "print('no. of documents with no metadata: {}'.format(no_documents_count))\n",
    "print('total no. of documents: {}'.format(len(gse_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of identifiers\n",
    "identifier_list  = []\n",
    "\n",
    "# appending the identifier entities\n",
    "identifier_list.extend(flat_list_identifier_ent)\n",
    "# appending the identifier ids\n",
    "identifier_list.extend(flat_list_identifier_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total no. of identifiers: {}'.format(len(identifier_list)))\n",
    "print('total no. of unique identifiers: {}'.format(len(set(identifier_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique identifiers for our columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of unique identifiers\n",
    "unique_identifier = set(identifier_list)\n",
    "# list of unique identifiers\n",
    "columns_unique_identifier = list(unique_identifier)\n",
    "\n",
    "#columns_unique_identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the identifier and their index dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifier and their index dict\n",
    "ident_idx_dict = {}\n",
    "\n",
    "# getting the index and values\n",
    "for idx, val in enumerate(columns_unique_identifier):\n",
    "    #print(idx, val)\n",
    "    \n",
    "    # value as key and index as values\n",
    "    ident_idx_dict[val] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ident_idx_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gse and their identifier and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gse as keys and identifier's index as values\n",
    "gse_ident_idx = {}\n",
    "\n",
    "# iterating over \n",
    "for gse, ident in document_id_and_identifier.items():\n",
    "    # keys as Gse id\n",
    "    gse_ident_idx[gse] = []\n",
    "    \n",
    "    # iterating over the identifiers\n",
    "    for ids in ident:\n",
    "        # appending the indexes of identifiers as values \n",
    "        gse_ident_idx[gse].append(ident_idx_dict[ids])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gse_ident_idx['GSE18877'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document_id_and_identifier['GSE18877'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Matrix with rows as GSE ids and columns as identifiers\n",
    "### Filling the matrix with 1's and 0's a/c to the presence or absence of identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of chunks of 100 gse ids for faster processing\n",
    "# chunks_valid_geo_ids = [valid_geo_ids[x:x+100] for x in range(0, len(valid_geo_ids), 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping the log\n",
    "# log_file = 'gse_md_fast_matrix_new_final.log'\n",
    "\n",
    "# #configuring the log file\n",
    "# logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - level=logging.INFO %(message)s')\n",
    "\n",
    "# #creating an empty dataframe\n",
    "# full_metadata = pd.DataFrame()\n",
    "\n",
    "# #total time to create the matrix\n",
    "# total_start = time.time()\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# #iterating over the list of chunks\n",
    "# for chunk_no in range(len(chunks_valid_geo_ids)):\n",
    "       \n",
    "#     #creating a dataframe with columns as unique identifier and index as chunks(100 gse ids)\n",
    "#     dataset_metadata_identifier_df = pd.DataFrame(columns=columns_unique_identifier, index=chunks_valid_geo_ids[chunk_no])\n",
    "    \n",
    "#     print('\\nchunk no. {} of {}\\n'.format(chunk_no + 1, len(chunks_valid_geo_ids)))\n",
    "      \n",
    "#       #iterating over 100 gse ids in this chunk\n",
    "#     for gse in chunks_valid_geo_ids[chunk_no]:#chunks_valid_geo_ids[0][0:10]:##\n",
    "#         #time to fill this 100 gse ids matrix\n",
    "#         start = time.time()\n",
    "#         count += 1\n",
    "\n",
    "#         #print('{} {}'.format(count, gse))\n",
    "#         #keeping all the columns as 0's\n",
    "#         zarray = np.zeros(len(columns_unique_identifier))\n",
    "#         #indexes where identifiers are present for this gse id are replaced with 1's\n",
    "#         zarray[gse_ident_idx[gse]] = 1\n",
    "\n",
    "\n",
    "#         #gse_vec = sparse.coo_matrix(zarray)#zarray\n",
    "#         #print(gse_vec.toarray())\n",
    "#         #gse_vec = pd.Series(zarray)\n",
    "\n",
    "#         #gse_vec = gse_vec.to_sparse()#zarray\n",
    "#         gse_vec = zarray\n",
    "#         #print(len(gse_vec))\n",
    "#         #print(len(dataset_metadata_identifier_df.loc[gse]))\n",
    "#         #replaced the value of the gse index as the above 0's and 1's vector \n",
    "#         dataset_metadata_identifier_df.loc[gse] = gse_vec#.toarray()\n",
    "#         #logging the info\n",
    "#         logging.info(str(count) + ' ' + gse + ' ' + 'Done')\n",
    "#     #appending the above 100 gse ids dataframe to the full metadata\n",
    "#     full_metadata = full_metadata.append(dataset_metadata_identifier_df)\n",
    "#     print('\\nshape of the full metadata matrix: {}\\n'.format(full_metadata.shape))\n",
    "#     #end time for creating 100 gse ids dataframe \n",
    "#     end = time.time()\n",
    "#     #end time for creating 100 gse ids dataframe\n",
    "\n",
    "#     print('\\ntime taken for {} gse id {}'.format(len(chunks_valid_geo_ids[chunk_no]), str(timedelta(seconds=elapsed))))\n",
    "#     print('-------------------------------------------\\n')\n",
    "# #end time for creating all the gse ids \n",
    "# total_end = time.time()\n",
    "# #total time taken for all gse ids dataframe\n",
    "# total_elapsed = total_end - total_start\n",
    "\n",
    "# print('\\ntotal time taken for {} {}'.format(len(valid_geo_ids), str(timedelta(seconds=total_elapsed))))\n",
    "#     #     if count > 5:\n",
    "#     #         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. of identifiers present per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_metadata.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(set(full_metadata.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"gse_metadata_fast_matrix_new_final\"\n",
    "# fileObject = open(file_Name,'wb') \n",
    "\n",
    "# pickle.dump(full_metadata, fileObject, protocol=4)  \n",
    "# fileObject.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open(file_Name,'rb')\n",
    "\n",
    "full_metadata = pickle.load(fileObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape of the full metadata matrix: {}'.format(full_metadata.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getsizeof(full_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to sparse dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_full_metadata = full_metadata.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getsizeof(sparse_full_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_Name = \"gse_metadata_fast_matrix_new_final_sparse\"\n",
    "# fileObject = open(file_Name,'wb') \n",
    "\n",
    "# pickle.dump(full_metadata, fileObject, protocol=4)  \n",
    "# fileObject.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. of identifiers present per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_metadata.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gse id for KAT2A and GCN5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gse_KAT2A = full_metadata.loc[full_metadata['UNIPROT:Q92830:T116:PRGE'] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gse_GCN5 = full_metadata.loc[full_metadata['UNIPROT:Q92831:T116:PRGE'] == 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gse_KAT2A_GCN5 = list(list(gse_KAT2A) + list(gse_GCN5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(gse_KAT2A_GCN5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling the matrix with 1's and 0's a/c to the presence or absence of identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #log_file = 'gse_md_fast_matrix.log'\n",
    "# #logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - level=logging.INFO %(message)s')\n",
    "\n",
    "# count = 0\n",
    "# #from scipy import sparse\n",
    "# total_start = time.time()\n",
    "# for gse in valid_geo_ids[100:200]:#chunks_valid_geo_ids[0][0:10]:##\n",
    "    \n",
    "#     start = time.time()\n",
    "#     count += 1\n",
    "    \n",
    "#     print(count)\n",
    "\n",
    "#     zarray = np.zeros(len(columns_unique_identifier))\n",
    "    \n",
    "#     zarray[gse_ident_idx[gse]] = 1\n",
    "    \n",
    "    \n",
    "#     #gse_vec = sparse.coo_matrix(zarray)#zarray\n",
    "#     #print(gse_vec.toarray())\n",
    "#     #gse_vec = pd.Series(zarray)\n",
    "    \n",
    "#     #gse_vec = gse_vec.to_sparse()#zarray\n",
    "#     gse_vec = zarray\n",
    "#     #print(len(gse_vec))\n",
    "#     #print(len(dataset_metadata_identifier_df.loc[gse]))\n",
    "#     dataset_metadata_identifier_df.loc[gse] = gse_vec#.toarray()\n",
    "    \n",
    "#     end = time.time()\n",
    "#     elapsed = end - start\n",
    "    \n",
    "#     print(str(timedelta(seconds=elapsed)))\n",
    "#     #logging.info(str(count) + ' ' + gse + ' ' + 'Done')\n",
    "    \n",
    "# total_end = time.time()\n",
    "# total_elapsed = total_end - total_start\n",
    "\n",
    "# print(str(timedelta(seconds=total_elapsed)))\n",
    "# #     if count > 5:\n",
    "# #         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_file = 'gse_md_matrix.log'\n",
    "# logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - level=logging.INFO %(message)s')\n",
    "\n",
    "# count = 0\n",
    "# #from scipy import sparse\n",
    "# total_start = time.time()\n",
    "# for gse in valid_geo_ids:#\n",
    "    \n",
    "#     start = time.time()\n",
    "#     count += 1\n",
    "    \n",
    "#     print(count)\n",
    "    \n",
    "   \n",
    "\n",
    "#     zarray = np.zeros(len(columns_unique_identifier))\n",
    "    \n",
    "#     zarray[gse_ident_idx[gse]] = 1\n",
    "    \n",
    "    \n",
    "#     #gse_vec = sparse.coo_matrix(zarray)#zarray\n",
    "    \n",
    "#     gse_vec = zarray\n",
    "    \n",
    "#     dataset_metadata_identifier_df.loc[gse] = gse_vec\n",
    "    \n",
    "#     end = time.time()\n",
    "#     elapsed = end - start\n",
    "    \n",
    "#     print(str(timedelta(seconds=elapsed)))\n",
    "#     logging.info(str(count) + ' ' + gse + ' ' + 'Done')\n",
    "    \n",
    "# total_end = time.time()\n",
    "# total_elapsed = total_end - total_start\n",
    "\n",
    "# print(str(timedelta(seconds=total_elapsed)))\n",
    "# #     if count > 5:\n",
    "# #         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_Name = \"gse_metadata_fast_matrix\"\n",
    "# fileObject = open(file_Name,'wb') \n",
    "\n",
    "# pickle.dump(dataset_metadata_identifier_df, fileObject)  \n",
    "# fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "\n",
    "# for i in range(len(valid_geo_ids)):#\n",
    "#     #try:\n",
    "#     start = time.time()\n",
    "#     count += 1\n",
    "#     print(count)\n",
    "#     row_name = dataset_metadata_identifier_df.index[i]\n",
    "#     #print(row_name)\n",
    "#     #if row_name == 'GSE95770' or row_name == 'GSE37642':\n",
    "#         #continue\n",
    "#     for j in range(len(columns_unique_identifier)):\n",
    "        \n",
    "#         col_name = columns_unique_identifier[j]\n",
    "#         #print(col_name)\n",
    "         \n",
    "#         if col_name in document_id_and_identifier[row_name]:\n",
    "#             dataset_metadata_identifier_df.loc[row_name, col_name] = 1\n",
    "#         #else:\n",
    "#             #dataset_metadata_identifier_df.loc[row_name, col_name] = 0\n",
    "#     end = time.time()\n",
    "#     elapsed = end - start\n",
    "#     #print(elapsed)\n",
    "#     print(str(timedelta(seconds=elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_metadata_identifier_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No. of identifiers present per dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_metadata_identifier_df.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_metadata_identifier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #full_metadata = pd.DataFrame()\n",
    "# full_metadata = full_metadata.append(dataset_metadata_identifier_df)\n",
    "# full_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata.loc['GSE33199'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id_and_identifier['GSE33199']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(document_id_and_identifier['GSE33199'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gse_ident_idx['GSE33199']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(gse_ident_idx['GSE33199'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the curated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_data = pd.read_csv('genequery_results/genequery_results/CuratedDataAML.csv')\n",
    "\n",
    "curated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the signature 'GSE12417' as user signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_signature = pd.read_csv('genequery_results/genequery_results/Sig_GSE12417_query1_AML.txt.conversion.csv')\n",
    "user_signature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the gene query results for the user signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sig_gene_query_results = pd.read_csv('genequery_results/genequery_results/Sig_GSE12417_query1_AML.txt.result.csv',sep = '\\t')\n",
    "user_sig_gene_query_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the top 'n' gene query results to aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gene_query_gse_results = user_sig_gene_query_results['GSE']\n",
    "gene_query_gse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def top_n_gene_query_results(n, gene_query_gse_results):\n",
    "    \n",
    "#     gene_query_gse_results_list = []\n",
    "#     count = 0\n",
    "    \n",
    "#     for gse in gene_query_gse_results:\n",
    "         \n",
    "#         gse_id = gse.split('\\t')[0]\n",
    "\n",
    "#         gene_query_gse_results_list.append(gse_id)\n",
    "        \n",
    "#         count += 1\n",
    "        \n",
    "#         if count >= n:\n",
    "#             break\n",
    "            \n",
    "#     return gene_query_gse_results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_query_gse_results_list = top_n_gene_query_results(5, gene_query_gse_results)\n",
    "# gene_query_gse_results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-n gene query results with the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_query_gse_results = user_sig_gene_query_results['GSE']\n",
    "# gene_query_gse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata given by user\n",
    "#user_metadata = ['AML', 'hematopoietic stem cells']\n",
    "user_metadata = ['AML']\n",
    "\n",
    "# list of identifiers of the metadata given by user\n",
    "metadata_ids_list = []\n",
    "\n",
    "# iterating over metadata given by users\n",
    "for metadata in user_metadata:\n",
    "    \n",
    "    # filling the list with the identifiers\n",
    "    metadata_ids_list.extend(merged_name_id_dict[metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_gene_query_results(n, gene_query_gse_results, metadata_ids_list):\n",
    "    \n",
    "    # list of top-n gene query o/p's having users metadata if present else will take the gene query o/p's \n",
    "    gene_query_gse_results_list = []\n",
    "    count = 0\n",
    "    \n",
    "    # iterating over the results of dataset given by gene query\n",
    "    for gse in gene_query_gse_results:\n",
    "        \n",
    "        # getting the gse id\n",
    "        gse_id = gse.split('\\t')[0]\n",
    "        \n",
    "        count_meta = 0\n",
    "        \n",
    "        # iterating over metdata ids given by users\n",
    "        for ids in metadata_ids_list:\n",
    "             \n",
    "            try:\n",
    "                # if this metadata is present for this gse ids \n",
    "                if full_metadata.loc[gse_id][ids] == 1:\n",
    "                    print('{} is present in {}'.format(list(merged_name_id_dict.keys())[list(merged_name_id_dict.values()).index(ids)], gse_id))\n",
    "                    count_meta += 1\n",
    "                    \n",
    "            except KeyError:\n",
    "                print('This {} id is not in our matrix'.format(gse_id))\n",
    "        \n",
    "        # if count meta is >= 1 means at least user's one metadata is present so we can take this gse id as one of top-n\n",
    "        if count_meta >= 1: #len(metadata_ids_list):\n",
    "            gene_query_gse_results_list.append(gse_id)\n",
    "        \n",
    "            count += 1\n",
    "            if count >= n:\n",
    "                break\n",
    "                \n",
    "    print('\\n{} GSE ids from gene query have at least any one of the metadata present\\n'.format(count))\n",
    "    \n",
    "    # if user's metadata is present in only few GQ o/p's or completely absent then we will take the top-n GQ results\n",
    "    if len(gene_query_gse_results_list) != n:\n",
    "        #print(gene_query_gse_results_list)\n",
    "        for gse in gene_query_gse_results:\n",
    "\n",
    "            if gse not in gene_query_gse_results_list:\n",
    "                gene_query_gse_results_list.append(gse)\n",
    "                #print(gene_query_gse_results_list)\n",
    "                print('added {} from gene query without user metadata in our top-{}'.format(gse, n))\n",
    "                if len(gene_query_gse_results_list) >= n:\n",
    "                    \n",
    "                    break\n",
    "        \n",
    "    return gene_query_gse_results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_query_gse_results_list = top_n_gene_query_results(5, gene_query_gse_results, metadata_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_query_gse_results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Getting the metadata for gene query results gse ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id_and_identifier_gene_query = {}\n",
    "\n",
    "flat_list_identifier_ent_gene_query = []\n",
    "flat_list_identifier_ids_gene_query = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_count_gene_query = 0\n",
    "no_documents_count_gene_query = 0\n",
    "\n",
    "valid_geo_ids_gene_query = []\n",
    "invalid_geo_ids_gene_query = []\n",
    "\n",
    "# iterating over the GQ o/p's got from above\n",
    "for gse_id in gene_query_gse_results_list:\n",
    "    \n",
    "    try:\n",
    "        # getting the metadata\n",
    "        get_metadata(gse_id, flat_list_identifier_ent_gene_query, flat_list_identifier_ids_gene_query, document_id_and_identifier_gene_query)\n",
    "        documents_count_gene_query+=1\n",
    "        valid_geo_ids_gene_query.append(gse_id)\n",
    "        print('getting metadata from {}'.format(gse_id))\n",
    "    \n",
    "    except TypeError:\n",
    "        \n",
    "        no_documents_count_gene_query+=1\n",
    "        invalid_geo_ids_gene_query.append(gse_id)\n",
    "        print('no metadata from {}'.format(gse_id))\n",
    "        \n",
    "    except ValueError:\n",
    "        \n",
    "        no_documents_count_gene_query+=1\n",
    "        invalid_geo_ids_gene_query.append(gse_id)\n",
    "        print('no metadata from {}'.format(gse_id))\n",
    "        \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_id_and_identifier_gene_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_list_identifier_ids_gene_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flat_list_identifier_ids_gene_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('no. of documents with metadata: {}'.format(documents_count_gene_query))\n",
    "print('no. of documents with no metadata: {}'.format(no_documents_count_gene_query))\n",
    "print('total no. of documents: {}'.format(len(gene_query_gse_results_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_list_gene_query  = []\n",
    "\n",
    "identifier_list_gene_query.extend(flat_list_identifier_ent_gene_query)\n",
    "identifier_list_gene_query.extend(flat_list_identifier_ids_gene_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifier_list_gene_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total no. of identifiers: {}'.format(len(identifier_list_gene_query)))\n",
    "print('total no. of unique identifiers: {}'.format(len(set(identifier_list_gene_query))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating the metadata of top n datasets from gene query and creating a vector w.r.t presence or absence of metadata and appending in our matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row to append in our existing metadata matrix \n",
    "row_name = 'user'\n",
    "\n",
    "# iterating over valid geo ids\n",
    "for i in valid_geo_ids_gene_query:#range(len(valid_geo_ids_gene_query))\n",
    "    \n",
    "    # iterating over the columns\n",
    "    for j in range(len(columns_unique_identifier)):\n",
    "        \n",
    "        # name of the column\n",
    "        col_name = columns_unique_identifier[j]\n",
    "        #print(col_name)\n",
    "\n",
    "        #if col_name in document_id_and_identifier_gene_query[i]:\n",
    "        # if column(identifier) is present in GQ identifier list \n",
    "        if col_name in identifier_list_gene_query:\n",
    "            full_metadata.loc[row_name, col_name] = 1\n",
    "        else:\n",
    "            full_metadata.loc[row_name, col_name] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_metadata.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking the users provided metadata and incorporating in our user row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user's metadata\n",
    "#user_metadata = ['AML', 'hematopoietic stem cells']\n",
    "user_metadata = ['AML']\n",
    "\n",
    "# metadata identifier list \n",
    "metadata_ids_list = []\n",
    "\n",
    "for metadata in user_metadata:\n",
    "    # getting the identifier of the metadata\n",
    "    metadata_ids_list.extend(merged_name_id_dict[metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over metadata ids list\n",
    "for metadata in metadata_ids_list:\n",
    "    # make the users column name(identifier) = 1 \n",
    "    full_metadata.loc[row_name][metadata] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_metadata_identifier_df = pd.DataFrame(columns=columns_unique_identifier, index=valid_geo_ids)\n",
    "full_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_metadata.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# cosine similarity b/w two vectors \n",
    "# def cos_sim(vec1, vec2):\n",
    "#     #cos_sim = cosine_similarity(metadata_df.loc['S10'].reshape(1,-1), metadata_df.loc['S1'].reshape(1, -1), dense_output=True)\n",
    "#     cos_sim = cosine_similarity(vec1, vec2)\n",
    "#     return cos_sim\n",
    "\n",
    "def cos_sim(df, vec1):\n",
    "    \n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    #cos_sim = cosine_similarity(metadata_df.loc['S10'].reshape(1,-1), metadata_df.loc['S1'].reshape(1, -1), dense_output=True)\n",
    "    cos_sim = cosine_similarity(df, vec1)\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating similarity of all datasets with user dataset w.r.t metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # name of the datasets\n",
    "# list_of_datasets = list(full_metadata.index)\n",
    "\n",
    "# # similarity score list\n",
    "# sim_score = []\n",
    "# #sign = []\n",
    "# datasets_list = []\n",
    "\n",
    "# sim_sign_dict_new = {}\n",
    "# # name of the user dataset\n",
    "# user_dataset = row_name\n",
    "# #user_sign = 'user'\n",
    "\n",
    "# # iterating over all the datasets\n",
    "# for i in list_of_datasets:\n",
    "    \n",
    "#     # take the similarity score b/w the user's matadata and other gse ids metadata\n",
    "#     if i != user_dataset:\n",
    "#         #print(dataset_metadata_identifier_df.loc[list_of_datasets[i]].values)\n",
    "#         similarity = cos_sim(full_metadata.loc[i].values.reshape(1,-1), full_metadata.loc[user_dataset].values.reshape(1, -1))\n",
    "#         #similarity = jaccard_similarity_score(full_metadata.loc[list_of_datasets[i]].values.reshape(1,-1), full_metadata.loc[user_dataset].values.reshape(1, -1))\n",
    "#         #print(similarity)\n",
    "#         datasets_list.append(i)\n",
    "#         sim_score.append(similarity[0][0])\n",
    "#         #sim_score.append(similarity)\n",
    "        \n",
    "#         sim_sign_dict_new[i] = similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the datasets\n",
    "list_of_datasets = list(full_metadata.index)\n",
    "\n",
    "# similarity score list\n",
    "sim_score = []\n",
    "\n",
    "# name of the user dataset\n",
    "user_dataset = row_name\n",
    "\n",
    "# # iterating over all the datasets\n",
    "# for i in range(len(list_of_datasets)):\n",
    "    \n",
    "#     # take the similarity score b/w the user's matadata and other gse ids metadata\n",
    "#     if i != list_of_datasets.index(user_dataset):\n",
    "#         #print(dataset_metadata_identifier_df.loc[list_of_datasets[i]].values)\n",
    "#         similarity = cos_sim(full_metadata.loc[list_of_datasets[i]].values.reshape(1,-1), full_metadata.loc[user_dataset].values.reshape(1, -1))\n",
    "#         #similarity = jaccard_similarity_score(full_metadata.loc[list_of_datasets[i]].values.reshape(1,-1), full_metadata.loc[user_dataset].values.reshape(1, -1))\n",
    "#         #print(similarity)\n",
    "        \n",
    "#         # appending the similarity score in a list \n",
    "#         sim_score.append(similarity[0][0])\n",
    "#         #sim_score.append(similarity)\n",
    "\n",
    "cosine_sim = cos_sim(full_metadata, full_metadata.loc[user_dataset].values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dict for dataset and their similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sim_sign_dict = {}\n",
    "\n",
    "# for i in range(len(sim_score)):\n",
    "#     sim_sign_dict[list_of_datasets[i]] = sim_score[i]\n",
    "\n",
    "sim_sign_dict = {}\n",
    "    \n",
    "count = -1\n",
    "\n",
    "for gse in full_metadata.index:\n",
    "\n",
    "    if gse != user_dataset:\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        sim_sign_dict[gse] = cosine_sim[count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sign_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorted Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_sim_score_dict(sim_sign_dict):\n",
    "    \n",
    "    sorted_score = sorted(sim_sign_dict.items(), key=lambda value: value[1], reverse=True)\n",
    "    return sorted_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_sim_score = sorted_sim_score_dict(sim_score)\n",
    "sort_sim_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K neighbors of similar datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_neighbors(sorted_score, k):\n",
    "    \n",
    "    top_k = sorted_score[0:k]\n",
    "    \n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = top_k_neighbors(sort_sim_score, 15)\n",
    "top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering from Top-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_k(top_k):\n",
    "    \n",
    "    filtered_top_k =[]\n",
    "    \n",
    "    for i in range(len(top_k)):\n",
    "        \n",
    "        if top_k[i][1] > 0.1:\n",
    "            \n",
    "            filtered_top_k.append(top_k[i])\n",
    "            \n",
    "    return filtered_top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top datasets to recommend based on only metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_top_k = filter_top_k(top_k)\n",
    "filtered_top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genequery results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_query_gse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = full_metadata.loc['GSE16236'][full_metadata.loc['GSE16236'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in f.index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the datasets and the similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dataset = [x[0] for x in filtered_top_k]\n",
    "sim_scr = [x[1] for x in filtered_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Curated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curated_sign = curated_data['Signature'].values\n",
    "\n",
    "# curated_gse_ids = []\n",
    "\n",
    "# for sig in curated_sign:\n",
    "    \n",
    "#     curated_gse_ids.append(sig.split('_')[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(curated_gse_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(curated_sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows_columns_ratings = curated_data.iloc[:,2:].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_ratings_df = pd.DataFrame(columns=gse_list[0], index=gse_list[0])\n",
    "# dataset_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_ratings_df = pd.DataFrame(columns=full_metadata.index, index=full_metadata.index)\n",
    "# dataset_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curated_data_preprocess = curated_data.iloc[:,1:]\n",
    "# curated_data_preprocess = curated_data_preprocess.set_index('Signature')\n",
    "# curated_data_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping only 'query1' datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sig in curated_data_preprocess.index:\n",
    "#     #print(sig)\n",
    "#     query = sig.split('_')[2]\n",
    "#     #print(query)\n",
    "    \n",
    "#     if query != 'query1':\n",
    "#         curated_data_preprocess.drop(sig, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing ratings format (0 is low and 5 is high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curated_data_preprocess = curated_data_preprocess.replace([1, 2, 4, 5], [5, 4, 2, 1])\n",
    "\n",
    "# curated_data_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gse in curated_data_preprocess.index:\n",
    "    \n",
    "#     gse_id = gse.split('_')[1]\n",
    "    \n",
    "#     dataset_ratings_df.loc[gse_id] = curated_data_preprocess.loc[gse]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curated_data_preprocess.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_ratings_df = dataset_ratings_df.fillna(0)\n",
    "#dataset_ratings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [('GSE24797', 0.2672612419124244),\n",
    "#  ('GSE28823', 0.2342606428329091),\n",
    "#  ('GSE27513', 0.20851441405707477)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'GSE24797' in dataset_ratings_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_score_list = []\n",
    "# dataset_names = dataset_ratings_df.columns\n",
    "\n",
    "# for i in range(0, len(dataset_ratings_df.columns)):\n",
    "#     score = []\n",
    "#     #print(dataset_names[i])\n",
    "#     for j in range(len(sim_dataset)):\n",
    "#         #print(sim_dataset[j])\n",
    "#         rating = dataset_ratings_df.loc[sim_dataset[j]][dataset_names[i]] \n",
    "#         score.append(rating*sim_scr[j])\n",
    "        \n",
    "#     total_score = np.nansum(score)/sum(sim_scr)\n",
    "#     total_score_list.append(total_score)\n",
    "#     #print('{0}:{1}'.format(dataset_names[i], total_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dict for dataset and there scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_score_dict = {}\n",
    "\n",
    "# for i in range(len(dataset_ratings_df.columns)):\n",
    "#     #print(str(i+1))\n",
    "#     dataset_score_dict[dataset_names[i]] = total_score_list[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting the dataset with their scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sorted_dataset_score_dict(dataset_scores):\n",
    "    \n",
    "#     sorted_dataset_score = sorted(dataset_scores.items(), key=lambda value: value[1], reverse=True)\n",
    "#     return sorted_dataset_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_score = sorted_dataset_score_dict(dataset_score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the top n datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def top_n_dataset(sorted_score, k):\n",
    "#     top_k = sorted_score[0:k]\n",
    "    \n",
    "#     return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_n_dataset = top_n_dataset(sorted_score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_n_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df = pd.DataFrame(np.random.randint(0, 2, (3, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df.loc[2].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim = cosine_similarity(df, df.loc[2].values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sign_new_dict = {}\n",
    "count = -1\n",
    "\n",
    "for gse in df.index:\n",
    "    \n",
    "    if gse != 2:\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "        sim_sign_new_dict[gse] = cos_sim[count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_sign_new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim[cos_sim.shape[1] - 1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
